....................................................................................
			  IO apis
....................................................................................
1.file system io
   file system io , how to read data from disk file
2.network io


File System IO:
=>We can read and write files from the disk in  two ways
  1.blocking way
  2.nonblocking way
=>We can read and write files using two mode
  1.NonStreaming mode
  2.Streaming mode
=>All file operations are handled by
  "Worker Threads" from Worker Thread Pool - either it is blocking or non blocking    io.
=>Files are handled using callback style or promise style.
=>Files operations are handled by "fs" module

.....................................................................................
			 File system operations
....................................................................................

How to read File using nonblocking pattern? using callbacks

fs.readFile(path[, options], callback)

path <string> | <Buffer> | <URL> | <integer> filename or file descriptor
options <Object> | <string>
 encoding <string> | <null> Default: null
 flag <string> See support of file system flags. Default: 'r'.
 signal <AbortSignal> allows aborting an in-progress readFile

callback <Function>
  err <Error> | <AggregateError>
  data <string> | <Buffer>

const fs = require('fs')

const blockMe = message => console.log(message)
//read file 
const filePath = './src/assets/info.txt'
const options = {
    encoding: 'UTF-8'
}
blockMe('start')
fs.readFile(filePath, options, (err, data) => {
    if (err) throw err
    console.log(data)
})
blockMe('end')


How to read file using promises? Using custom promise

const fs = require('node:fs')

//read file 
const filePath = './src/assets/info.txt'
const options = {
    encoding: 'UTF-8'
}

class FileOperation {
    constructor() { }
    // readFile(callback) {
    //     fs.readFile(filePath, options, callback)
    // }
    readFile() {
        return new Promise((resolve, reject) => {
            fs.readFile(filePath, options, (err, data) => {
                if (err) {
                    reject(err``)
                }
                resolve(data)
            })
        })
    }
}

async function main() {
    let fileOperation = new FileOperation()
    // fileOperation.readFile((err, data) => {
    //     if (err) throw err;
    //     console.log(data)
    // })
    // fileOperation.readFile()
    //     .then(data => console.log(data))
    //     .catch(err => console.log(err))
    try {
        const data = await fileOperation.readFile()
        console.log(data)
    }
    catch (err) {
        console.log(err)
    }
}
main()
...............................
Reading file using Built Promise api

const filePromise = require('node:fs/promises');

async function logFile() {
    try {
        const filePath = './src/assets/info.txt'
        const contents = await filePromise.readFile(filePath, { encoding: 'utf8' });
        console.log(contents);
    } catch (err) {
        console.error(err.message);
    }
}
logFile();
.....................................................................................

How to read File using blocking pattern?

const fs = require('node:fs')

const path = './src/assets/info.txt'

const options = {
    encoding: 'UTF-8'
}

function blockMe(message) {
    console.log(message)
}
//async api to read file
blockMe('start')
const data = fs.readFileSync(path, options)
console.log(data)
blockMe('end')


How to use path module? and global variables


The path module provides utilities for working with file and directory paths.

-node provides lot of global variables

__dirname  : current directory name
C:\session\ibm\2021\june\nodemicroservices\nodeapps\src

__filename :current directory name + fileName
C:\session\ibm\2021\june\nodemicroservices\nodeapps\src\index.js
.........................

const fs = require('fs');
const path = require('path')

// const filePath = './src//assets/info.txt'
const filePath = path.join(__dirname,'assets/info.txt')

const options = {
    encoding: 'UTF-8'
}

function blockMe(message) {
    console.log(message)
}
//async api to read file
blockMe('start')
fs.readFile(filePath, options, (err, data) => {
    if (err) throw err
    console.log(data)
})
blockMe('end')
.....................................................................................

How to write data into file using nonblocking and blocking pattern?

NonBlocking write using callbacks:

const fs = require('node:fs')
const path = require('node:path')

function write() {
    let filePath = path.join(__dirname, 'assets/content.txt')
    let options = {
        encoding: 'UTF-8'
    }
    let content = 'Hello, this is a node.js file write example'
    fs.writeFile(filePath, content, options, (err) => {
        if (err) {
            console.log('Error writing file :', err)
            return;
        }
        console.log('File has been written successfully!')
    })
}

function main() {
    write()
}
main()

Write data into file using Promise api:
const fs = require('node:fs/promises')
const path = require('node:path')

async function write() {
    let filePath = path.join(__dirname, 'assets/content.txt')
    let options = {
        encoding: 'UTF-8'
    }
    let content = 'Hello, this is a node.js file write example'
    try {
        await fs.writeFile(filePath, content, options)
        console.log('File has been written successfully')
    }
    catch (err) {
        console.log(err)
    }
}

async function main() {
    await write()
}
main()

Task: How to write json data 
.........................................................................................

.....................................................................................
Mode of fs read and write:
.........................

1.Non Streaming Mode

2.Streaming  Mode


1.Non Streaming Mode

  only file io is supported, network io not supported

 -once file is read, the entire file is loaded into node process buffer(memory), then it will be delivered to caller.

-if more files are loaded into node process, node process gets crashed.

-non streaming mode is not suitable for large and big files read or write operation.

fs.readFile() and fs.writeFile are non streaming apis.


2.Streaming apis:
   supported by fs and also network apis


-Streaming is nothing but flow of data(chunks).
-Streaming allows move the data from one place to another place one by one.
-Streaming apis are other wise called evented io. which is powered events.


Types of Streams:

1.Readable Stream : input
2.Writeable stream : output
3.Duplex stream : read + write

Node has lot of built in stream apis
....................................

Built in readable Streams:

-HTTP responses, on the client
-HTTP requests, on the server
-fs read streams
-zlib streams
-crypto streams
-TCP sockets
-child process stdout and stderr
-process.stdin

Writable Streams:

-HTTP requests, on the client
-HTTP responses, on the server
-fs write streams
-zlib streams
-crypto streams
-TCP sockets
-child process stdin
-process.stdout, process.stderr


All streaming apis are powered with events
node io streams has built in events.
events are emitted by node.
Our programs are listeners


Common events in all io
.........................


1.data event:
 which is emitted by node, for each chunk.

2.close event:
  The 'close' event is emitted when the stream and any of its underlying resources (a file descriptor, for example) have been closed.

3.end event:
 The 'end' event is emitted when there is no more data to be consumed from the stream.

4.Event: 'error'
 The 'error' event may be emitted by a Readable implementation at any time
Typically, this may occur if the underlying stream is unable to generate data due to an underlying internal failure, or when a stream implementation attempts to push an invalid chunk of data.
..........................................................................................

Fs Read Stream:
const fs = require('node:fs')
const path = require('node:path')

function read() {
    let filePath = path.join(__dirname, 'assets/info.txt')
    let options = {
        encoding: 'UTF-8'
    }
    const inputStream = fs.createReadStream(filePath, options)

    //attach stream listeners
    let data = ''
    inputStream.on('data', (chunk) => {
        //
        data += chunk
    })
    inputStream.on('end', () => {
        console.log('there is no more')
        console.log(data)
    })
    inputStream.on('close', () => {
        console.log('close ')
    })
    inputStream.on('error', (err) => {
        console.log(err)
    })
}

function main() {
    read()
}
main()

Fs Write Stream:
const fs = require('node:fs')
const path = require('node:path')
const todos = require('./mock-data/todos')

function write() {
    let filePath = path.join(__dirname, 'assets/todos.json')
    const config = {
        encoding: 'utf8',
        flag: 'w'
    };
    const outputStream = fs.createWriteStream(filePath, config)
    //data
    let jsonTodos = JSON.stringify(todos)
    outputStream.write(jsonTodos)
    //attach events

    outputStream.close();
    outputStream.on('close', function () {
        console.log('file has been written ')
    })


}

function main() {
    write()
}
main()
.....................................................................................
				Back Pressure
....................................................................................

When input stream and output stream works together.

Backpressure:

  Problems when you do read and write together

1. In general read operation is faster than write operation


Back Pressure means inputstream is fast, outputstream slow, then data will be
lost.


How to handle back pressure?

 apis  : pause,resume,drain event

pause : to close the upstream, not to emit data
resume : to open the open upstream , to emit data
drain event: if drain event is called, means buffer is empty


in order to test back pressure, we need big files

Create big file code:
//big file creation
const fs = require('fs');
const path = require('path')

const filePath = path.join(__dirname, "assets/big.file")

const file = fs.createWriteStream(filePath);

for (let i = 0; i <= 1e6; i++) {
    file.write('Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n');
}

file.end();
.................
How to handle back pressure using pause,resume,drain?

const fs = require('fs');
const path = require('path');

const inputfileName = path.join(__dirname, 'assets/big.file');
const outputfileName = path.join(__dirname, 'assets/bigcopy.file');

const config = {
    encoding: 'UTF-8'
}
const readerStream = fs.createReadStream(inputfileName, config);
const writeStr = fs.createWriteStream(outputfileName, config);


readerStream.on('data', function (chunk) {
    console.log(`Received ${chunk.length} bytes of data.`);
    let buffer_good = writeStr.write(chunk);
    if (!buffer_good) readerStream.pause();
});

writeStr.on('drain', function () {
    console.log('buffer drained!');
    readerStream.resume();
});
readerStream.on('end', function () {
    //console.log(data);
});

readerStream.on('error', function (err) {
    console.log(err.stack);
});	

.....................................................................................
	   Pipe method to eliminate backpressure apis(drain,resume,pause)
...................................................................................
const fs = require('fs');
const path = require('path');

const inputfileName = path.join(__dirname, 'assets/big.file');
//write
const outputFileName = path.join(__dirname, 'assets/bigcopy.file');

const config = {
      encoding: 'UTF-8'
}

//Back pressure handling
const readerStream = fs.createReadStream(inputfileName, config);
const writeStr = fs.createWriteStream(outputFileName, config);

//backPressure streams
//pipe method is simplest method which wraps resume,pasuse,drain 
readerStream.pipe(writeStr);

.....................................................................................


